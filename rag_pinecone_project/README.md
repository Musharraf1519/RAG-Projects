# ğŸ§  BrainWave: RAG Q&A with Pinecone

BrainWave is a **Retrieval-Augmented Generation (RAG) system** that allows users to **ask natural language questions** on large documents. It retrieves relevant information using **Pinecone vector database** and generates human-like answers using **open-source LLMs**.  

This project demonstrates a **production-ready modular RAG pipeline** suitable for interviews, research assistance, knowledge management, and document Q&A.

---

## **Features**

- ğŸ“‚ Upload and ingest multiple documents (PDF, DOCX, TXT)
- ğŸ§© Automatic **chunking** of large documents
- âš¡ Generate **semantic embeddings** using `sentence-transformers`
- ğŸ’¾ Store and query embeddings in **Pinecone vector database**
- ğŸ¤– Answer queries using **Flan-T5 open-source LLM**
- ğŸŒ Interactive **Streamlit web interface**
- ğŸ”§ Modular, production-ready code

---

## **Folder Structure**

rag_pinecone_project/<br>
â”œâ”€â”€ app.py # Streamlit frontend<br>
â”œâ”€â”€ requirements.txt # Python dependencies<br>
â”œâ”€â”€ data/ # Uploaded documents stored here<br>
â”œâ”€â”€ rag/<br>
â”‚ â”œâ”€â”€ ingestion.py # Load documents<br>
â”‚ â”œâ”€â”€ chunking.py # Split text into chunks<br>
â”‚ â”œâ”€â”€ embedding.py # Generate embeddings<br>
â”‚ â”œâ”€â”€ vector_store.py # FAISS wrapper with incremental indexing<br>
â”‚ â”œâ”€â”€ retrieval.py # Retrieve top-k relevant chunks<br>
â”‚ â””â”€â”€ generation.py # LLM-based answer generation<br>

---


## **Installation**

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/rag_pinecone_project.git
cd rag_pinecone_project
```
2. **Create and activate a virtual environment**
```bash
python -m venv venv
# Windows
venv\Scripts\activate
# Linux/Mac
source venv/bin/activate
```
3. **Install dependencies**
```bash
pip install -r requirements.txt
```
4. **Set Pinecone API key**
* Temporary (current session)
```bash
set PINECONE_API_KEY=your_api_key
```
* Permanent (all future sessions)
```bash
setx PINECONE_API_KEY "your_api_key"
```
### Close and reopen terminal

--- 

## **Usage**

1. Run Streamlit app
```bash
streamlit run app.py
```
2. Upload Documents
* Use the Upload Documents section to upload .pdf, .docx, or .txt files.
* Documents are saved automatically to the data/ folder.
3. Ingest & Index Documents
* Click ğŸš€ Ingest & Index Documents to:
    * Chunk documents
    * Generate embeddings
    * Upsert embeddings into Pinecone index
4. Ask Questions
* Enter a natural language query in the text input.
* The system will retrieve relevant chunks and generate a human-like answer.

---

## **Pipeline Overview**

1. Document Ingestion â€“ Load files into memory
2. Chunking â€“ Split large text into smaller overlapping chunks
3. Embedding â€“ Convert chunks into semantic vectors
4. Vector DB (Pinecone) â€“ Store embeddings for fast semantic search
5. Query Augmentation & Retrieval â€“ Fetch top-k relevant chunks
6. Generation (LLM) â€“ Generate natural language answer using context

---

## **Dependencies**

* streamlit â€“ Web interface
* sentence-transformers â€“ Embedding generation
* transformers â€“ LLM for answer generation
* torch â€“ Backend for transformers
* pinecone â€“ Vector database
* PyPDF2 â€“ PDF reading
* python-docx â€“ DOCX reading

---

## **Future Improvements**

* Add .env support for API key
* Support more LLMs (CPU-friendly or GPU-based)
* Add document metadata search (author, date, etc.)
* Include logging & progress bars for ingestion
* Dockerize for easy deployment

---
## **ğŸ“¸ Screenshots**

![Landing Page](screenshots/landing.png)
![Upload Document](screenshots/upload.png)
![Ingest Documents](screenshots/ingest.png)
![Query & Answer](screenshots/query.png)

---

## ğŸ‘¨â€ğŸ’» Author

<b>Musharraf Hussain Khan</b><br>
[GitHub](https://github.com/Musharraf1519)<br>
[LinkedIn](https://www.linkedin.com/in/musharraf-hussain-khan/)<br>
Email: musharrafhussainkhann@example.com
